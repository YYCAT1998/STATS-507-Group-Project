{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I2hlNAbzHOj",
        "colab_type": "text"
      },
      "source": [
        "# **507 final project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5tkw2capO9r",
        "colab_type": "text"
      },
      "source": [
        "First, save the data on google drive and mount to google drive, so we can use the data in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX5fM_9MmmfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duJC9iUUmvxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -r 'drive/My Drive/507_project/plant2/.' train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWr-tqXDlU9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from os import listdir, path\n",
        "import os\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms.transforms import Compose, Resize, ToTensor, CenterCrop\n",
        "import time\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage import util\n",
        "from skimage.filters import gaussian\n",
        "from skimage import img_as_ubyte\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms,models\n",
        "from torch.optim import lr_scheduler\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhK-5ILtz90D",
        "colab_type": "text"
      },
      "source": [
        "Implement data pre-precessing and split the data into train, test, valid datasets randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4D_AEaldRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createdir(root_dir, target_dir):\n",
        "    all_labels = listdir(path.join(root_dir, 'train'))\n",
        "    all_data = []\n",
        "    \n",
        "    # Create a list of tuples out of data samples. Each tuple includes images file name and an int as label code\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        os.makedirs(path.join(target_dir, parent_dir))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHDPr2p-ldTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "createdir('.','./new/train')\n",
        "createdir('.','./new/test')\n",
        "createdir('.','./new/valid')\n",
        "createdir('.','./new/origin_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LejZGuL2lcxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 256\n",
        "IMG_SIZE1 = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nig19dKyldVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ew_colormin=(25,50,50)\n",
        "new_colormax=(80,255,255)\n",
        "\n",
        "def plot_mask(image, colormin, colormax):\n",
        "        hsv_p1 = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)   #R, G, and B are converted to the floating-point format and scaled to fit the 0 to 1 range\n",
        "        mask = cv2.inRange(hsv_p1, colormin , colormax)  #?\n",
        "        result = cv2.bitwise_and(image, image, mask=mask)\n",
        "        return result\n",
        "\n",
        "\n",
        "def rotate(image, angle=90, scale=1.0):\n",
        "    '''\n",
        "    Rotate the image\n",
        "    :param image: image to be processed\n",
        "    :param angle: Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).\n",
        "    :param scale: Isotropic scale factor.\n",
        "    '''\n",
        "    w = image.shape[1]\n",
        "    h = image.shape[0]\n",
        "    #rotate matrix\n",
        "    M = cv2.getRotationMatrix2D((w/2,h/2), angle, scale)\n",
        "    #rotate\n",
        "    image = cv2.warpAffine(image,M,(w,h))\n",
        "    return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbk3xCNHldZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(root_dir, train_dir, val_dir, test_dir, origin_train_dir):\n",
        "    all_labels = listdir(path.join(root_dir, 'train'))\n",
        "    all_data = []\n",
        "    target_size = 1500\n",
        "    # Create a list of tuples out of data samples. Each tuple includes images file name and an int as label code\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        name_list = []\n",
        "        parent_directory = path.join(root_dir, 'train', parent_dir)\n",
        "        file_list = listdir(parent_directory)\n",
        "        random.shuffle(file_list)\n",
        "        for test_sample in file_list[:40]:\n",
        "            img = cv2.imread(path.join(parent_directory, test_sample))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = plot_mask(img, new_colormin, new_colormax)\n",
        "\n",
        "            cv2.imwrite(path.join(test_dir, parent_dir, test_sample), img)\n",
        "        for val_sample in file_list[40:80]:\n",
        "            img = cv2.imread(path.join(parent_directory, val_sample))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = plot_mask(img, new_colormin, new_colormax)\n",
        "\n",
        "            cv2.imwrite(path.join(val_dir, parent_dir, val_sample), img)\n",
        "        origin_size = len(file_list) - 80\n",
        "        choose = min(int((target_size - origin_size)/3), origin_size)\n",
        "        for addi in file_list[80:80+choose]:\n",
        "            angle = np.random.uniform(45,180,1)\n",
        "            scale = np.random.uniform(0.8,1.2,1)\n",
        "            img = cv2.imread(path.join(parent_directory, addi))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = plot_mask(img, new_colormin, new_colormax)\n",
        "\n",
        "            flipped_img = np.fliplr(img)\n",
        "            flipped_img2 = np.flipud(img)\n",
        "            rotated_img = rotate(img, angle=angle, scale=scale)\n",
        "            #noised_img = skimage.util.random_noise(img, mode=\"poisson\")\n",
        "            cv2.imwrite(path.join(train_dir, parent_dir, addi), img)\n",
        "            cv2.imwrite(path.join(origin_train_dir, parent_dir, addi), img)\n",
        "\n",
        "            cv2.imwrite(path.join(train_dir, parent_dir, addi[:len(addi)-4]+\"_flip.png\"), flipped_img)\n",
        "            cv2.imwrite(path.join(train_dir, parent_dir, addi[:len(addi)-4]+\"_flipud.png\"), flipped_img2)\n",
        "            cv2.imwrite(path.join(train_dir, parent_dir, addi[:len(addi)-4]+\"_rotate.png\"), rotated_img)\n",
        "            #io.imsave(path.join(train_dir, parent_dir, addi[:len(addi)-4]+\"_noise.png\"), img_as_ubyte(noised_img))\n",
        "        for rest in file_list[80+choose:]:\n",
        "            img = cv2.imread(path.join(parent_directory, rest))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = plot_mask(img, new_colormin, new_colormax)\n",
        "\n",
        "            cv2.imwrite(path.join(origin_train_dir, parent_dir, rest), img)\n",
        "            cv2.imwrite(path.join(train_dir, parent_dir, rest), img)\n",
        "\n",
        "        if 4*origin_size < target_size:\n",
        "            choose2 = min(int(target_size-4*origin_size), origin_size)\n",
        "            for add2 in file_list[80:80+choose2]:\n",
        "                angle = np.random.uniform(45,180,1)\n",
        "                scale = np.random.uniform(0.8,1.2,1)\n",
        "                img = cv2.imread(path.join(parent_directory, add2))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = plot_mask(img, new_colormin, new_colormax)\n",
        "                rotated_img2 = rotate(img, angle=angle, scale=scale)\n",
        "                cv2.imwrite(path.join(train_dir, parent_dir, add2[:len(add2)-4]+\"_rotate2.png\"), rotated_img2)\n",
        "\n",
        "        if 5*origin_size < target_size:\n",
        "            choose3 = min(int(target_size-5*origin_size), origin_size)\n",
        "            for add3 in file_list[80:80+choose3]:\n",
        "                angle = np.random.uniform(45,180,1)\n",
        "                scale = np.random.uniform(0.8,1.2,1)\n",
        "                img = cv2.imread(path.join(parent_directory, add3))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = plot_mask(img, new_colormin, new_colormax)\n",
        "                rotated_img3 = rotate(img, angle=angle, scale=scale)\n",
        "                cv2.imwrite(path.join(train_dir, parent_dir, add3[:len(add3)-4]+\"_rotate3.png\"), rotated_img3)\n",
        "\n",
        "        if 6*origin_size < target_size:\n",
        "            choose4 = min(int(target_size-6*origin_size), origin_size)\n",
        "            for add4 in file_list[80:80+choose4]:\n",
        "                angle = np.random.uniform(45,180,1)\n",
        "                scale = np.random.uniform(0.8,1.2,1)\n",
        "                img = cv2.imread(path.join(parent_directory, add4))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = plot_mask(img, new_colormin, new_colormax)\n",
        "                rotated_img4 = rotate(img, angle=angle, scale=scale)\n",
        "                cv2.imwrite(path.join(train_dir, parent_dir, add4[:len(add4)-4]+\"_rotate4.png\"), rotated_img4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AtqDh2Oldba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_train_test('.', './new/train','./new/valid', './new/test', './new/origin_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moOTxHNRldXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDataLoaders_split(train_dir, train_transforms, batch_size):\n",
        "    all_labels = listdir(train_dir)\n",
        "    all_data = []\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        this_dir_images = glob(path.join(train_dir, parent_dir, '*.png'))\n",
        "        all_data += zip(this_dir_images, [label_code]*len(this_dir_images))\n",
        "    random.shuffle(all_data)\n",
        "    train_data = SeedlingDataset(all_data, len(all_labels), transform=train_transforms)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    return train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nqfqa5O0K0K",
        "colab_type": "text"
      },
      "source": [
        "The first model we use is a 4-layer CNN model. We bulid the model and train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChAYsUCfl5_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TinyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(3, 12, kernel_size = 5, stride = 1,padding = 2, bias = True), nn.BatchNorm2d(12), nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(12, 24, kernel_size = 5, stride = 1, bias = True), nn.BatchNorm2d(24), nn.ReLU())\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(24, 48, kernel_size = 5, stride = 1, bias = True), nn.BatchNorm2d(48), nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(48, 96, kernel_size = 5, stride = 1, bias = True), nn.BatchNorm2d(96), nn.ReLU())\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=9600, out_features=1200)\n",
        "        self.fc2 = nn.Linear(in_features=1200, out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=12)\n",
        "        self.pool=nn.MaxPool2d(2,stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape) \n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        x = self.pool(self.conv3(x))\n",
        "        x = self.pool(self.conv4(x))\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, prediction, true_values):\n",
        "\n",
        "        return F.nll_loss(prediction, true_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7NX2j99l9IE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a class for the dataset\n",
        "class SeedlingDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, n_labels, transform=None):\n",
        "        \n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.n_labels = n_labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        data_file, image_code = self.data[idx]\n",
        "        img = Image.open(data_file)\n",
        "        img = img.convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        img = torch.from_numpy(np.array(img))\n",
        "        label = torch.tensor(image_code)\n",
        "        \n",
        "        return img, label\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC0Qxcn1l9Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_dataloader, optimizer, epoch, verbose=False):\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (image, label) in enumerate(train_dataloader):\n",
        "        \n",
        "        input_var = image.to(device)\n",
        "        target_var = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_var)\n",
        "        loss = model.loss(output, target_var)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss\n",
        "        if batch_idx % 10 == 0 and verbose:\n",
        "            print('Train Epoch: {0}, Train batch: {1}, Batch loss: {2}'\n",
        "                  .format(epoch, batch_idx, loss))\n",
        "    \n",
        "    epoch_loss = total_loss/(batch_idx + 1)\n",
        "    \n",
        "    return epoch_loss\n",
        "    \n",
        "def valid(model, device, dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(dataloader):\n",
        "          input_var = image.to(device)\n",
        "          target_var = label.to(device)\n",
        "          output = model(input_var)\n",
        "          loss = model.loss(output, target_var)\n",
        "          total_loss += loss\n",
        "      \n",
        "        epoch_loss = total_loss/(batch_idx + 1)\n",
        "        # print('validation loss: ',epoch_loss)\n",
        "    return epoch_loss\n",
        "    \n",
        "def test(model, device, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_dataloader:\n",
        "            input_var = image.to(device)\n",
        "            target_var = label.to(device)\n",
        "            output = model(input_var)\n",
        "            prediction = output.argmax(dim=1, keepdim=True)\n",
        "            correct += prediction.eq(target_var.view_as(prediction)).sum().item()\n",
        "        accuracy = correct / len(test_dataloader.dataset)\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxeXdHb2l9TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AdjustLR(optim, lr):\n",
        "    '''\n",
        "    This is a simple hand-made function to decrease \n",
        "    optimizer's learning rate by a factor of 10 at a specific time\n",
        "    '''\n",
        "    for param_group in optim.param_groups:\n",
        "            param_group['lr'] /= 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMiQCfpAmFxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = []\n",
        "valid_losses = []\n",
        "accuracies = []\n",
        "test_accuracies = []\n",
        "n_epochs = 20\n",
        "learning_rate = 0.05\n",
        "lr_decay = 5 # every 10 epochs, the learning rate is divided by 10\n",
        "batch_size = 64\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "use_cuda = True # use True to switch to GPU\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = TinyModel().to(device)\n",
        "print('The model has {0} parameters'.format(sum([len(i.reshape(-1)) for i in model.parameters()]) ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9GRJ9lhmFz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "mean1 = [0.5, 0.5, 0.5]\n",
        "std1 = [0.5, 0.5, 0.5]\n",
        "\n",
        "test_transform = Compose([Resize(IMG_SIZE), CenterCrop(IMG_SIZE1), ToTensor(), transforms.Normalize(mean, std)])\n",
        "train_transform = Compose([\n",
        "    transforms.RandomResizedCrop(size=IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.CenterCrop(size=IMG_SIZE1),  # Image net standards\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)  # Imagenet standards\n",
        "])\n",
        "\n",
        "# valid_data = SeedlingDataset(valid_data_filename, len(all_labels), transform=test_transform)\n",
        "valid_loader = getDataLoaders_split('./new/valid', test_transform, batch_size = batch_size)\n",
        "\n",
        "# train_data = SeedlingDataset(train_data_filename, len(all_labels), transform=train_transform)\n",
        "train_loader = getDataLoaders_split('./new/train', train_transform, batch_size = batch_size)\n",
        "\n",
        "# test_data = SeedlingDataset(test_data_filename, len(all_labels), transform=test_transform)\n",
        "test_loader = getDataLoaders_split('./new/test', test_transform, batch_size = batch_size)\n",
        "\n",
        "print('number of train examples: {0}, number of validation examples: {1}, number of test examples: {2}'\n",
        "      .format(len(train_loader.dataset), len(test_loader.dataset), len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CIjQS12mF2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "    tic = time.time()\n",
        "    epoch_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "    print('Training loss for epoch {0} is {1:.5f}'.format(epoch, epoch_loss))\n",
        "    losses.append(epoch_loss)\n",
        "    valid_loss = valid(model, device, valid_loader)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print('Val loss: {0:.3f}'.format(valid_loss))\n",
        "    test_accuracy = test(model, device, test_loader)\n",
        "    print('Test accuracy: {0:.3f}'.format(test_accuracy))\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    tac = time.time()\n",
        "    print('Epoch time: {0:0.1f} seconds'.format(tac - tic))\n",
        "    if epoch % lr_decay == 0:\n",
        "        AdjustLR(optimizer, learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaS6ofgIqfl4",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the 4-layer CNN model based on loss and accuracy plot; confusion matrix and f1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt57Bul5mF43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "step = np.arange(1,21, dtype = int)\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Average Cross Entropy', color=color)\n",
        "ax1.plot(step, losses, label=\"Train loss\")\n",
        "ax1.plot(step,valid_losses, label=\"Valid loss\")\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "# color = 'tab:blue'\n",
        "# ax2.set_ylabel('Accuracy', color=color)\n",
        "# # ax2.set_ylim(0,1)  # we already handled the x-label with ax1\n",
        "# ax2.plot(step, test_accuracies, color=\"g\", label = \"Test Accuracy\")\n",
        "# ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "\n",
        "ax1.legend(loc=10)\n",
        "# ax2.legend(loc=7)\n",
        "plt.title('Validation& Training losses')\n",
        "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXwgA79zmdpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# model.eval()\n",
        "# nb_classes = 12\n",
        "# confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "# with torch.no_grad():\n",
        "#     for i, (inputs, classes) in enumerate(test_loader):\n",
        "#         inputs = inputs.to(device)\n",
        "#         classes = classes.to(device)\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "#             confusion_matrix[t.long(), p.long()] += 1\n",
        "# print(confusion_matrix)\n",
        "recall = confusion_matrix2.diag()/confusion_matrix2.sum(1) #recall\n",
        "precision = confusion_matrix2.diag()/confusion_matrix2.sum(0) #precision\n",
        "f1 = f1_score(classes.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
        "p = precision_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "r = recall_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "print(f1, p, r)\n",
        "confusion_matrix1 = torch.zeros(nb_classes, nb_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gg0omHxmTnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
        "#               'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
        "\n",
        "# abbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\n",
        "CATEGORIES = ['Common Chickweed',\n",
        " 'Small-flowered Cranesbill',\n",
        " 'Maize',\n",
        " 'Loose Silky-bent',\n",
        " 'Black-grass',\n",
        " 'Common wheat',\n",
        " 'Scentless Mayweed',\n",
        " 'Cleavers',\n",
        " 'Charlock',\n",
        " 'ShepherdGÇÖs Purse',\n",
        " 'Fat Hen',\n",
        " 'Sugar beet']\n",
        "abbreviation = ['CC', 'SFC', 'M', 'LSB', 'BG', 'CW', 'SM', 'Cl', 'Ch', 'SP', 'FH', 'SB']\n",
        "pd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})\n",
        "fig, ax = plt.subplots(1)\n",
        "ax = sns.heatmap(confusion_matrix2, ax=ax, cmap=plt.cm.Blues, annot=True)\n",
        "ax.set_xticklabels(abbreviation)\n",
        "ax.set_yticklabels(abbreviation)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "#fig.savefig('Confusion matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFLKtGApb4g",
        "colab_type": "text"
      },
      "source": [
        "Fit the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQDxRsgpgvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/plant_seedlings_classification\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3demJrz9pnV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224), \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Validation \n",
        "    'val':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Test\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KNrXpsTpnYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 64\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root='datasplitted1/train', transform=image_transforms['train']),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root='datasplitted1/val', transform=image_transforms['val']),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root='datasplitted1/test', transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM3pEhb3psm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = models.vgg16(pretrained=True)\n",
        "model = models.resnet152(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPM_efXfpspV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_class = len(data['train'].classes)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, n_class), nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnaBOsMdpsrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to('cuda')\n",
        "summary(model, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5gSZ0qIpstm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.class_to_idx = data['train'].class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(model.idx_to_class.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKebdyKQp5mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gWoE4IHp5tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_file_name = 'resnet152-transfer-3.pt'\n",
        "checkpoint_path = 'resnet152-transfer-3.pth'\n",
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20,\n",
        "          print_every=2):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(data)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwcqE0agp5xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_on_gpu = True\n",
        "model, history = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=40,\n",
        "    print_every=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVf4jQO0qUjq",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_3Z4Nap5zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(figsize=(16, 12))\n",
        "fig, ax1 = plt.subplots()\n",
        "color = 'tab:red'\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    ax1.plot(\n",
        "        history[c], label=c)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Average Cross Entropy')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "# for c in ['train_acc', 'valid_acc']:\n",
        "ax2.plot(history['train_acc'], label='train_acc', color='r')\n",
        "ax2.plot(history['valid_acc'], label='valid_acc', color='g')\n",
        "ax2.set_ylabel('Average Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax1.legend(loc = 10)\n",
        "ax2.legend(loc = 7)\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Average Cross Entropy')\n",
        "plt.title('Training and Validation Losses & Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_DR_z3nqGQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4HBLfOaqGix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "def cm_acc_ontest(model, test_data, device='cuda'):\n",
        "    # Initialize the prediction and label lists(tensors)\n",
        "    predlist=torch.zeros(0,dtype=torch.long)\n",
        "    lbllist=torch.zeros(0,dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i, (data, target) in enumerate(test_data):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Append batch prediction results\n",
        "            predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
        "            lbllist=torch.cat([lbllist,target.view(-1).cpu()])\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "\n",
        "    # Per-class accuracy\n",
        "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "    return conf_mat, class_accuracy\n",
        "\n",
        "# https://deeplizard.com/learn/video/0LhiS6yu2qQ\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Greens):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    plt.figure(figsize=(10,10))\n",
        "    # print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "classes = [\n",
        "    class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "]\n",
        "cm, acc = cm_acc_ontest(model, dataloaders['test'])\n",
        "plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additional Steps. Use both model A and model B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDataLoaders_split2(train_dir, train_transforms, batch_size, randomsh = True):\n",
        "    all_labels = listdir(train_dir)\n",
        "    all_data = []\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        this_dir_images = glob(path.join(train_dir, parent_dir, '*.png'))\n",
        "        all_data += zip(this_dir_images, [label_code]*len(this_dir_images))\n",
        "    if randomsh:\n",
        "        random.shuffle(all_data)\n",
        "    train_data = SeedlingDataset(all_data, len(all_labels), transform=train_transforms)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=randomsh, num_workers=0)\n",
        "    return train_loader\n",
        "# main\n",
        "from torchvision import datasets, transforms,models\n",
        "\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "mean1 = [0.5, 0.5, 0.5]\n",
        "std1 = [0.5, 0.5, 0.5]\n",
        "\n",
        "test_transform = Compose([Resize(IMG_SIZE), CenterCrop(IMG_SIZE1), ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "\n",
        "IMG_SIZE = 256\n",
        "IMG_SIZE1 = 224\n",
        "\n",
        "batch_size = 64\n",
        "use_cuda = True # use True to switch to GPU\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = TinyModel().to(device)\n",
        "\n",
        "model.load_state_dict(torch.load('D:/Yueyang/model_save/4_layer_model.pt'))\n",
        "model.eval() # model A\n",
        "\n",
        "nb_classes=12\n",
        "test_loader2 = getDataLoaders_split2('D:/yueyang/test', test_transform, batch_size = 480, randomsh = False)\n",
        "confusion_matrix_A = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(test_loader2):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        # Team member has different order, so need to resort.\n",
        "        preds[preds == 4] = 100+0\n",
        "        preds[preds == 8] = 100+1\n",
        "        preds[preds == 7] = 100+2\n",
        "        preds[preds == 0] = 100+3\n",
        "        preds[preds == 5] = 100+4\n",
        "        preds[preds == 10] = 100+5\n",
        "        preds[preds == 3] = 100+6\n",
        "        preds[preds == 2] = 100+7\n",
        "        preds[preds == 6] = 100+8\n",
        "        preds[preds == 9] = 100+9\n",
        "        preds[preds == 1] = 100+10\n",
        "        preds[preds == 11] = 100+11\n",
        "\n",
        "        preds = preds-100\n",
        "     \n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "            confusion_matrix_A[t.long(), p.long()] += 1\n",
        "print(confusion_matrix_A)\n",
        "\n",
        "f1 = f1_score(classes.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
        "p = precision_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "r = recall_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "print(f1, p, r)\n",
        "\n",
        "print(f1_score(classes.cpu(), preds.cpu(), average=None))\n",
        "print(precision_score(classes.cpu(), preds.cpu(), average=None))\n",
        "print(recall_score(classes.cpu(), preds.cpu(), average=None))\n",
        "\n",
        "#4,8,7,0,5,10,3,2,6,9,1,11\n",
        "#['Common Chickweed'0, 'Small-flowered Cranesbill'1, 'Maize'2, 'Loose Silky-bent'3, 'Black-grass'4, 'Common wheat'5, \n",
        "# 'Scentless Mayweed'6, 'Cleavers'7, 'Charlock'8, 'ShepherdGÇÖs Purse'9, 'Fat Hen'10, 'Sugar beet'11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pp = np.array(preds.cpu()).tolist()\n",
        "index_both = [index for index in range(len(pp)) if pp[index] == 0 or pp[index] == 6]\n",
        "\n",
        "model.load_state_dict(torch.load('D:/Yueyang/model_save/2_modle_10.pth'))\n",
        "model.eval()# model B\n",
        "\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    input_var = inputs[index_both]\n",
        "    target_var = classes[index_both]\n",
        "    output = model(input_var)\n",
        "    _, prediction = torch.max(output, 1)\n",
        "    correct = (target_var == prediction).sum().item()\n",
        "    accuracy = correct / input_var.shape[0]\n",
        "accuracy\n",
        "\n",
        "\n",
        "\n",
        "prediction[prediction == 1] = 6\n",
        "prediction[prediction == 0] = 0\n",
        "\n",
        "preds[index_both] = prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "confusion_matrix_B = torch.zeros(nb_classes, nb_classes)\n",
        "\n",
        "for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "    confusion_matrix_B[t.long(), p.long()] += 1\n",
        "print(confusion_matrix_B)\n",
        "print(confusion_matrix_B.diag()/confusion_matrix_B.sum(1))\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "f1 = f1_score(classes.cpu(), preds.cpu(), average='macro')\n",
        "p = precision_score(classes.cpu(), preds.cpu(), average='macro')\n",
        "r = recall_score(classes.cpu(), preds.cpu(), average='macro')\n",
        "print(f1) # another measure.\n",
        "print(p) # another measure.\n",
        "print(r) # another measure.\n",
        "\n",
        "print(f1_score(classes.cpu(), preds.cpu(), average=None))\n",
        "print(precision_score(classes.cpu(), preds.cpu(), average=None))\n",
        "print(recall_score(classes.cpu(), preds.cpu(), average=None))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f1 = f1_score(classes.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
        "p = precision_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "r = recall_score(classes.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n",
        "print(f1, p, r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
        "              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
        "\n",
        "abbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\n",
        "pd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})\n",
        "fig, ax = plt.subplots(1)\n",
        "ax = sns.heatmap(confusion_matrix_B, ax=ax, cmap=plt.cm.Blues, annot=True)\n",
        "ax.set_xticklabels(abbreviation)\n",
        "ax.set_yticklabels(abbreviation)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "fig.savefig('D:/507_plant_classification/cm_modelB.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following is to get model B "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_augmentation(root_dir):\n",
        "    all_labels = listdir(path.join(root_dir))\n",
        "    all_data = []\n",
        "    \n",
        "    # Create a list of tuples out of data samples. Each tuple includes images file name and an int as label code\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        parent_directory = path.join(root_dir, parent_dir)\n",
        "        file_list = listdir(parent_directory)\n",
        "        for file_name in file_list:\n",
        "            img = cv2.imread(path.join(parent_directory, file_name))\n",
        "            for i in range(36):\n",
        "                angle = 180/36*i + 16\n",
        "                scale = np.random.uniform(0.8,1.2,1)\n",
        "                rotated_img = rotate(img, angle=angle, scale=scale)\n",
        "                cv2.imwrite(path.join(parent_directory, file_name[:len(file_name)-4]+\"_rotate\"+str(i)+\".png\"), rotated_img)\n",
        "                \n",
        "#image_augmentation('D:/Yueyang/new_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms,models\n",
        "\n",
        "IMG_SIZE = 256\n",
        "IMG_SIZE1 = 224\n",
        "\n",
        "\n",
        "losses = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "n_epochs = 20\n",
        "learning_rate = 0.05\n",
        "lr_decay = 5 # every 10 epochs, the learning rate is divided by 10\n",
        "batch_size = 64\n",
        "use_cuda = True # use True to switch to GPU\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = TinyModel().to(device)\n",
        "print('The model has {0} parameters'.format(sum([len(i.reshape(-1)) for i in model.parameters()]) ))\n",
        "\n",
        "\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "mean1 = [0.5, 0.5, 0.5]\n",
        "std1 = [0.5, 0.5, 0.5]\n",
        "\n",
        "test_transform = Compose([Resize(IMG_SIZE), CenterCrop(IMG_SIZE1), ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "train_loader = getDataLoaders_split('D:/yueyang/new_train',test_transform, batch_size = batch_size)\n",
        "test_loader = getDataLoaders_split('D:/yueyang/2_test', test_transform, batch_size = batch_size)\n",
        "val_loader = getDataLoaders_split('D:/yueyang/new_val', test_transform, batch_size = batch_size)\n",
        "\n",
        "\n",
        "print('number of train examples: {0}, number of test examles: {1}'\n",
        "      .format(len(train_loader.dataset), len(test_loader.dataset)))\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    tic = time.time()\n",
        "    epoch_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "    print('Training loss for epoch {0} is {1:.5f}'.format(epoch, epoch_loss))\n",
        "    losses.append(epoch_loss)\n",
        "    val_accuracy = test(model, device, val_loader)\n",
        "    print('Val accuracy: {0:.3f}'.format(val_accuracy))\n",
        "    \n",
        "    val_accuracies.append(val_accuracy)\n",
        "    tac = time.time()\n",
        "    print('Epoch time: {0:0.1f} seconds'.format(tac - tic))\n",
        "    if val_accuracy>0.7:\n",
        "        torch.save(model.state_dict(), \"D:/0Yueyang/model_save/2_modle_\"+str(epoch)+\".pth\")\n",
        "\n",
        "    if epoch % lr_decay == 0:\n",
        "        AdjustLR(optimizer, learning_rate)\n",
        "\n",
        "        \n",
        "# use the complicated model, bs = 64, origin data, 82.5%\n",
        "# use the same model, bs = 32, aug data, 86.5%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "updated pretrained model part codes："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir, path\n",
        "import cv2\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch import optim, cuda\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "\n",
        "import split_folders\n",
        "split_folders.fixed('train', output=\"datasplitted1\", seed=1337, fixed=(40, 40), oversample=False)\n",
        "\n",
        "new_colormin=(25,50,50)\n",
        "new_colormax=(80,255,255)\n",
        "\n",
        "def plot_mask(image, colormin, colormax):\n",
        "        hsv_p1 = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)   \n",
        "        mask = cv2.inRange(hsv_p1, colormin , colormax)\n",
        "        result = cv2.bitwise_and(image, image, mask=mask)\n",
        "        #plt.figure(figsize=(15,10))\n",
        "        #plt.subplot(1, 3, 1)\n",
        "        #plt.imshow(image)\n",
        "        #plt.grid(False)\n",
        "        #plt.subplot(1, 3, 2)\n",
        "        #plt.imshow(mask, cmap=\"gray\")\n",
        "        #plt.grid(False)\n",
        "        #plt.subplot(1, 3, 3)\n",
        "        #plt.imshow(result)\n",
        "        #plt.grid(False)\n",
        "        return result\n",
        "\n",
        "def color_seg_rewrite(root_dir):\n",
        "    all_labels = listdir(path.join(root_dir))\n",
        "    all_data = []\n",
        "    \n",
        "    # Create a list of tuples out of data samples. Each tuple includes images file name and an int as label code\n",
        "\n",
        "    for label_code, parent_dir in enumerate(all_labels):\n",
        "        print(parent_dir)\n",
        "        parent_directory = path.join(root_dir, parent_dir)\n",
        "        file_list = listdir(parent_directory)\n",
        "        for file_name in file_list:\n",
        "            print(file_name)\n",
        "            img = cv2.imread(path.join(parent_directory, file_name))\n",
        "            \n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = plot_mask(img, new_colormin, new_colormax)\n",
        "            \n",
        "            cv2.imwrite(path.join(parent_directory, file_name), img)\n",
        "color_seg_rewrite('datasplitted1/train')\n",
        "color_seg_rewrite('datasplitted1/test')\n",
        "color_seg_rewrite('datasplitted1/val')\n",
        "\n",
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=30),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224), \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Validation \n",
        "    'val':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Test\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "batch_size = 16\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root='datasplitted1/train', transform=image_transforms['train']),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root='datasplitted1/val', transform=image_transforms['val']),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root='datasplitted1/test', transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
        "}\n",
        "\n",
        "model = models.resnet152(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_class = len(data['train'].classes)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, n_class), nn.LogSoftmax(dim=1))\n",
        "model = model.to('cuda')\n",
        "model.class_to_idx = data['train'].class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "save_file_name = 'resnet152-transfer-1.pt'\n",
        "checkpoint_path = 'resnet152-transfer-1.pth'\n",
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          scheduler,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=30,\n",
        "          device='cuda'):\n",
        "   \n",
        "\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Iterate over data.\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward\n",
        "            output = model(data)\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # stats\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    loss = criterion(output, target)\n",
        "\n",
        "                    # stats\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "                scheduler.step()\n",
        "\n",
        "                \n",
        "                print(\n",
        "                    f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                )\n",
        "                print(\n",
        "                    f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch}, loss: {valid_loss_min:.2f}, accuracy: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'total training time(s){total_time:.2f}. Time per epoch{total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history\n",
        "\n",
        "train_on_gpu = True\n",
        "model, history = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=40)\n",
        "\n",
        "# plt.figure(figsize=(16, 12))\n",
        "fig, ax1 = plt.subplots()\n",
        "color = 'tab:red'\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    ax1.plot(\n",
        "        history[c], label=c)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Average Cross Entropy')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "# for c in ['train_acc', 'valid_acc']:\n",
        "ax2.plot(history['train_acc'], label='train_acc', color='r')\n",
        "ax2.plot(history['valid_acc'], label='valid_acc', color='g')\n",
        "ax2.set_ylabel('Average Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax1.legend(loc = 10)\n",
        "ax2.legend(loc = 7)\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Average Cross Entropy')\n",
        "plt.title('Training and Validation Losses & Accuracy')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}